{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538c6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c59485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PrepareData import read_json, make_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b36cea",
   "metadata": {},
   "source": [
    "# EXP 1: Compare MCC and SCC over Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4bba3",
   "metadata": {},
   "source": [
    "## Read results from disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6802cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = sys.path[0].replace('notebooks', '')\n",
    "eval_path=repo_dir+ 'eval/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efad7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualization change the values to be consistent with the order that higher is better\n",
    "def normalize_fairness_measures(x):\n",
    "    if 'Diff' in x.iloc[0]: # difference change to 1-abs(x)\n",
    "        return 1-abs(x.iloc[1])\n",
    "    \n",
    "    elif x.iloc[0] == 'DI':\n",
    "        if x.iloc[1] > 1:\n",
    "            return min(x.iloc[1], 1/x.iloc[1])\n",
    "        else:\n",
    "            return x.iloc[1]\n",
    "    else:# other metrics\n",
    "        return x.iloc[1]\n",
    "def add_vis_flag(x):\n",
    "    if 'Diff' in x.iloc[0]: # difference change to 1-abs(x)\n",
    "        if x.iloc[0] in ['ERRDiff', 'FNRDiff', 'FPRDiff']: # measures with lower value means better\n",
    "            if x.iloc[1] > 0: \n",
    "                return 0\n",
    "            else: # G0 has better outcome\n",
    "                return 1\n",
    "        else: # for measures like eqdiff, avgoddsdiff with higher value means better\n",
    "            if x.iloc[1] < 0: \n",
    "                return 0\n",
    "            else: # G0 has better outcome\n",
    "                return 1\n",
    "    \n",
    "    elif x.iloc[0] == 'DI':\n",
    "        if x.iloc[1] > 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:# other metrics\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23aea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 12345, 6, 2211, 15]\n",
    "\n",
    "models = ['LR', 'TR']\n",
    "\n",
    "datasets = ['cardio', 'credit', 'ACSH', 'ACSI', 'ACSM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76870e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save evaluation results at /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/eval/mcc_5_n5_res.csv\n"
     ]
    }
   ],
   "source": [
    "# eval_suffix = '-min_g0-0.5'\n",
    "eval_suffix = 'res'\n",
    "# eval_suffix = 'res-min'\n",
    "eval_file = 'mcc_{}_n{}_{}.csv'.format(len(datasets), len(seeds), eval_suffix)\n",
    "if os.path.exists(eval_path+eval_file):\n",
    "    eval_df = pd.read_csv(eval_path+eval_file)\n",
    "    print('Read evaluation results at {}'.format(eval_path+eval_file))\n",
    "else:\n",
    "    eval_df = pd.DataFrame()\n",
    "    for data_name in datasets:\n",
    "        cur_eval_df = pd.read_csv(eval_path+'{}-{}.csv'.format(eval_suffix, data_name))\n",
    "#         print(cur_eval_df.head())\n",
    "        cur_eval_df['norm_value'] = cur_eval_df[['metric', 'value']].apply(lambda x: normalize_fairness_measures(x), axis=1)\n",
    "        cur_eval_df['norm_flag'] = cur_eval_df[['metric', 'value']].apply(lambda x: add_vis_flag(x), axis=1)\n",
    "\n",
    "        eval_df = pd.concat([eval_df, cur_eval_df])\n",
    "    \n",
    "    eval_df.to_csv(eval_path+eval_file, index=False)\n",
    "    print('Save evaluation results at {}'.format(eval_path+eval_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a717c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>seed</th>\n",
       "      <th>method</th>\n",
       "      <th>group</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>norm_value</th>\n",
       "      <th>norm_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cardio</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>all</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cardio</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>all</td>\n",
       "      <td>ACC</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cardio</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>all</td>\n",
       "      <td>SR</td>\n",
       "      <td>0.338476</td>\n",
       "      <td>0.338476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cardio</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>all</td>\n",
       "      <td>BalAcc</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cardio</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>G0</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0.617000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     data model  seed   method group  metric     value  norm_value  norm_flag\n",
       "0  cardio    LR     1  MCC-MIN   all     AUC  0.609000    0.609000          0\n",
       "1  cardio    LR     1  MCC-MIN   all     ACC  0.610000    0.610000          0\n",
       "2  cardio    LR     1  MCC-MIN   all      SR  0.338476    0.338476          0\n",
       "3  cardio    LR     1  MCC-MIN   all  BalAcc  0.609000    0.609000          0\n",
       "4  cardio    LR     1  MCC-MIN    G0     AUC  0.617000    0.617000          0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced9c48",
   "metadata": {},
   "source": [
    "## Draw barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e9b9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plots(df, output_name, vis_datasets, vis_metric, vis_settings, group_input=None,\n",
    "              legend_names=None, font_label=26, font_legend=18, \n",
    "              colors=['#ffffff', '#fffacd', '#3cb371','#20603d', '#0e6670'], bg_color = '#f3f3f3', x_tick_offset=6.3,\n",
    "              x_ticks=None, y_label=None, x_label=None, legend=True, legend_col=5, save_to_disc=True):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(10, 4), dpi=200)\n",
    "    input_df = df.copy()\n",
    "    bar_mean = []\n",
    "    bar_std = []\n",
    "    dash_filling = []\n",
    "    line_styles = []\n",
    "    x_bars = []\n",
    "    ind = 0\n",
    "    \n",
    "    for off_i, name in enumerate(vis_datasets):\n",
    "        vis_df = input_df[(input_df['data']==name) & (input_df['metric']==vis_metric) & (input_df['group']==group_input)].copy()\n",
    "        for setting_i in vis_settings:\n",
    "            set_df = vis_df[vis_df['method']==setting_i]\n",
    "            \n",
    "            if set_df.shape[0] > 0:\n",
    "                y_values = np.array(set_df['norm_value'])\n",
    "                n_reverse = sum(np.array(set_df['norm_flag']))\n",
    "                if n_reverse > int(len(y_values) * 0.9): # majoirty of cases in which G0 has better outcomes\n",
    "                    dash_filling.append(True)\n",
    "                else:\n",
    "                    dash_filling.append(False)\n",
    "                \n",
    "                cur_mean = np.mean(y_values)\n",
    "                cur_std = np.std(y_values)\n",
    "#                 if data_name == 'lsac' and setting_i == 'OMN-ONE':\n",
    "#                     print(data_name, cur_mean, cur_std)\n",
    "                if cur_mean == 0:\n",
    "#                     print('++', name, setting_i, cur_mean)\n",
    "                    cur_mean = 0.01 # for visualization purpose so that the bar exists in the plot\n",
    "                    line_styles.append('solid')\n",
    "                elif vis_metric == 'BalAcc' and cur_std < 0.01 and abs(cur_mean-0.5) < 0.1:\n",
    "                    # dashed border\n",
    "                    line_styles.append('dashed')\n",
    "                elif vis_metric in ['DI', 'AvgOddsDiff', 'EQDiff'] and cur_std < 0.01 and abs(1-cur_mean) < 0.0001:\n",
    "                    line_styles.append('dashed')\n",
    "                    cur_mean = 0.001\n",
    "                else:\n",
    "                    line_styles.append('solid')\n",
    "                    \n",
    "                bar_mean.append(cur_mean)\n",
    "                bar_std.append(cur_std)\n",
    "            else: # no model is returned\n",
    "                dash_filling.append(False)\n",
    "                line_styles.append('dashed')\n",
    "                bar_mean.append(0)\n",
    "                bar_std.append(0)\n",
    "                \n",
    "            x_bars.append(ind+off_i*2)\n",
    "                \n",
    "            ind += 0.83\n",
    "    bplot = ax.bar(x_bars, bar_mean, yerr=bar_std)\n",
    "#     print('-->', vis_metric, bar_mean)\n",
    "    \n",
    "    n_bars = len(vis_settings)\n",
    "    for idx, patch in enumerate(bplot):\n",
    "        patch.set_facecolor(colors[idx % n_bars])\n",
    "        \n",
    "        if dash_filling[idx]:\n",
    "            patch.set_hatch('//')\n",
    "            patch.set_edgecolor(\"#cb4154\")\n",
    "        else:\n",
    "            patch.set_edgecolor(\"black\")\n",
    "        patch.set_linestyle(line_styles[idx])\n",
    "        \n",
    "    if legend_names:\n",
    "        legends = legend_names\n",
    "    else:\n",
    "        legends = vis_settings\n",
    "    # add labels for settings \n",
    "    for idx, setting_i, color_i, legend_i in zip(range(len(vis_settings)), vis_settings, colors, legends):\n",
    "        ax.bar(-2, 1, ec='black', fc=color_i, label=legend_i)\n",
    "\n",
    "    ax.set_facecolor(bg_color)\n",
    "    ax.yaxis.grid(True)\n",
    "    plt.xlim([-1, max(x_bars)+1])\n",
    "    plt.xticks([(x-1)*x_tick_offset+0.8 for x in range(1, len(vis_datasets)+1)])\n",
    "    if x_ticks:\n",
    "        locs, labels=plt.xticks();\n",
    "        plt.xticks(locs, x_ticks, horizontalalignment='center', fontsize=font_label-10, rotation=0);\n",
    "\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.yticks(fontsize=font_label);\n",
    "\n",
    "    if y_label:\n",
    "        plt.ylabel(y_label, fontsize=font_label)\n",
    "\n",
    "    if x_label:\n",
    "        plt.xlabel(x_label, fontsize=font_label)\n",
    "\n",
    "    if legend:\n",
    "        plt.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower center\", mode=\"expand\", ncol=legend_col, frameon=False, borderaxespad=0, handlelength=0.9, handletextpad=0.3, fontsize=font_label-7)\n",
    "\n",
    "    if save_to_disc:\n",
    "        plt.savefig(output_name, bbox_inches=\"tight\")\n",
    "        print('Bar plot is saved at ', output_name)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef33f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = repo_dir+ 'intermediate/plots/mcc/'\n",
    "make_folder(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2f3422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/mcc/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebec91f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/mcc/LR-mcc-DI.png\n"
     ]
    }
   ],
   "source": [
    "# eval_suffix = 'mcc'\n",
    "\n",
    "exp_datasets = datasets\n",
    "exp_ticks = ['Cardio', 'Credit', 'ACSH', 'ACSI', 'ACSM']\n",
    "\n",
    "mcc_settings = ['ORIG', 'SEP', 'MCC-W2', 'SCC-KAM']\n",
    "mcc_legends = ['ORIG', 'SEP', 'MCC-W2', 'SCC+K']\n",
    "mcc_colors = ['#ffffff', '#fffacd', '#0e6670', '#2e8b57']\n",
    "\n",
    "\n",
    "eval_metrics = ['BalAcc', 'DI', 'AvgOddsDiff', 'EQDiff', 'FPRDiff', 'FNRDiff', 'ERRDiff'][1:2]\n",
    "\n",
    "x_tick_set = 5.6\n",
    "for model_name in models[:1]:\n",
    "        \n",
    "    vis_df = eval_df.query('model==\"{}\"'.format(model_name))\n",
    "    \n",
    "    for exp_metric in eval_metrics:\n",
    "        output_name = '{}{}-{}-{}.png'.format(plot_path, model_name, 'mcc', exp_metric)\n",
    "        bar_plots(vis_df, output_name, exp_datasets, exp_metric, mcc_settings, group_input='all', x_ticks=exp_ticks, colors=mcc_colors, \n",
    "                  legend_names=mcc_legends, x_tick_offset=x_tick_set,\n",
    "                  legend_col=len(mcc_settings),\n",
    "                  save_to_disc=True)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303725d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
