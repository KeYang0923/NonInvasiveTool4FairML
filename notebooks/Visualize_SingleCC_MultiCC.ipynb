{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538c6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c59485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from PrepareData import read_json, make_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b36cea",
   "metadata": {},
   "source": [
    "# EXP 1: Comparing to baselines: no intervention and multi-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a4bba3",
   "metadata": {},
   "source": [
    "## Read results from disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6802cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = sys.path[0].replace('notebooks', '')\n",
    "eval_path=repo_dir+ 'eval/'\n",
    "# data_name = 'lsac'\n",
    "# temp_df = pd.read_csv(eval_path+'res-{}.csv'.format(data_name))\n",
    "# temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efad7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for visualization change the values to be consistent with the order that higher is better\n",
    "def normalize_fairness_measures(x):\n",
    "    if 'Diff' in x.iloc[0]: # difference change to 1-abs(x)\n",
    "        return 1-abs(x.iloc[1])\n",
    "    \n",
    "    elif x.iloc[0] == 'DI':\n",
    "        if x.iloc[1] > 1:\n",
    "            return min(x.iloc[1], 1/x.iloc[1])\n",
    "        else:\n",
    "            return x.iloc[1]\n",
    "    else:# other metrics\n",
    "        return x.iloc[1]\n",
    "def add_vis_flag(x):\n",
    "    if 'Diff' in x.iloc[0]: # difference change to 1-abs(x)\n",
    "        if x.iloc[0] in ['ERRDiff', 'FNRDiff', 'FPRDiff']: # measures with lower value means better\n",
    "            if x.iloc[1] > 0: \n",
    "                return 0\n",
    "            else: # G0 has better outcome\n",
    "                return 1\n",
    "        else: # for measures like eqdiff, avgoddsdiff with higher value means better\n",
    "            if x.iloc[1] < 0: \n",
    "                return 0\n",
    "            else: # G0 has better outcome\n",
    "                return 1\n",
    "    \n",
    "    elif x.iloc[0] == 'DI':\n",
    "        if x.iloc[1] > 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:# other metrics\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23aea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 12345, 6, 2211, 15, 88, 121, 433, 500, 1121, 50, 583, 5278, 100000, 0xbeef, 0xcafe, 0xdead, 7777, 100, 923]\n",
    "# seeds = [88, 121, 433, 500, 1121, 50, 583, 5278, 100000, 0xbeef, 0xcafe, 0xdead, 7777, 100, 923]\n",
    "\n",
    "models = ['LR', 'TR']\n",
    "\n",
    "datasets = ['lsac', 'cardio', 'bank', 'meps16', 'credit', 'ACSE', 'ACSP', 'ACSH', 'ACSM', 'ACSI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76870e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save evaluation results at /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/eval/scc_mcc_datasets10_n20-min_g0-0.5.csv\n"
     ]
    }
   ],
   "source": [
    "eval_suffix = '-min_g0-0.5'\n",
    "# eval_suffix = ''\n",
    "eval_file = 'scc_mcc_datasets{}_n{}{}.csv'.format(len(datasets), len(seeds), eval_suffix)\n",
    "if os.path.exists(eval_path+eval_file):\n",
    "    eval_df = pd.read_csv(eval_path+eval_file)\n",
    "else:\n",
    "    eval_df = pd.DataFrame()\n",
    "    for data_name in datasets:\n",
    "        cur_eval_df = pd.read_csv(eval_path+'res{}-{}.csv'.format(eval_suffix, data_name))\n",
    "        cur_eval_df['norm_value'] = cur_eval_df[['metric', 'value']].apply(lambda x: normalize_fairness_measures(x), axis=1)\n",
    "        cur_eval_df['norm_flag'] = cur_eval_df[['metric', 'value']].apply(lambda x: add_vis_flag(x), axis=1)\n",
    "\n",
    "        eval_df = pd.concat([eval_df, cur_eval_df])\n",
    "    \n",
    "    eval_df.to_csv(eval_path+eval_file, index=False)\n",
    "    print('Save evaluation results at {}'.format(eval_path+eval_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a717c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>seed</th>\n",
       "      <th>method</th>\n",
       "      <th>group</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>norm_value</th>\n",
       "      <th>norm_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>all</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>all</td>\n",
       "      <td>ACC</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>all</td>\n",
       "      <td>SR</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0.272059</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>all</td>\n",
       "      <td>BalAcc</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC-MIN</td>\n",
       "      <td>G0</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data model  seed   method group  metric     value  norm_value  norm_flag\n",
       "0  lsac    LR     1  MCC-MIN   all     AUC  0.625000    0.625000          0\n",
       "1  lsac    LR     1  MCC-MIN   all     ACC  0.625000    0.625000          0\n",
       "2  lsac    LR     1  MCC-MIN   all      SR  0.272059    0.272059          0\n",
       "3  lsac    LR     1  MCC-MIN   all  BalAcc  0.625000    0.625000          0\n",
       "4  lsac    LR     1  MCC-MIN    G0     AUC  0.625000    0.625000          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6909b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>model</th>\n",
       "      <th>seed</th>\n",
       "      <th>method</th>\n",
       "      <th>group</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>norm_value</th>\n",
       "      <th>norm_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>ACC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>SR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>1</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>BalAcc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>lsac</td>\n",
       "      <td>LR</td>\n",
       "      <td>12345</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7327</th>\n",
       "      <td>lsac</td>\n",
       "      <td>TR</td>\n",
       "      <td>100</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>BalAcc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>lsac</td>\n",
       "      <td>TR</td>\n",
       "      <td>923</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>AUC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7525</th>\n",
       "      <td>lsac</td>\n",
       "      <td>TR</td>\n",
       "      <td>923</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>ACC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7526</th>\n",
       "      <td>lsac</td>\n",
       "      <td>TR</td>\n",
       "      <td>923</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>SR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>lsac</td>\n",
       "      <td>TR</td>\n",
       "      <td>923</td>\n",
       "      <td>SCC-KAM</td>\n",
       "      <td>G0</td>\n",
       "      <td>BalAcc</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      data model   seed   method group  metric  value  norm_value  norm_flag\n",
       "124   lsac    LR      1  SCC-KAM    G0     AUC    0.0         0.0          0\n",
       "125   lsac    LR      1  SCC-KAM    G0     ACC    NaN         NaN          0\n",
       "126   lsac    LR      1  SCC-KAM    G0      SR    NaN         NaN          0\n",
       "127   lsac    LR      1  SCC-KAM    G0  BalAcc    0.0         0.0          0\n",
       "304   lsac    LR  12345  SCC-KAM    G0     AUC    0.0         0.0          0\n",
       "...    ...   ...    ...      ...   ...     ...    ...         ...        ...\n",
       "7327  lsac    TR    100  SCC-KAM    G0  BalAcc    0.0         0.0          0\n",
       "7524  lsac    TR    923  SCC-KAM    G0     AUC    0.0         0.0          0\n",
       "7525  lsac    TR    923  SCC-KAM    G0     ACC    NaN         NaN          0\n",
       "7526  lsac    TR    923  SCC-KAM    G0      SR    NaN         NaN          0\n",
       "7527  lsac    TR    923  SCC-KAM    G0  BalAcc    0.0         0.0          0\n",
       "\n",
       "[160 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.query('method == \"SCC-KAM\" and data==\"lsac\" and group==\"G0\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d95a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48895e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fa6de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracting evalaution results from different methods with multiple models\n",
    "# res_path = 'intermediate/models/'\n",
    "# set_suffix = 'S_1'\n",
    "\n",
    "\n",
    "# group_eval_metrics = ['AUC', 'ACC', 'SR', 'BalAcc']\n",
    "# overall_metrics = ['BalAcc', 'DI', 'EQDiff', 'AvgOddsDiff', 'SPDiff', 'FPRDiff', 'FNRDiff', 'ERRDiff']\n",
    "\n",
    "# methods = ['MCC-MIN', 'MCC-W1', 'MCC-W2', 'SEP', 'ORIG']\n",
    "\n",
    "# eval_path = 'intermediate/evals/'\n",
    "# eval_file = 'Data{}-run{}-{}-{}.csv'.format(len(datasets), len(seeds), set_suffix, 'multi')\n",
    "\n",
    "\n",
    "# all_eval_files = list(filter(lambda x: not os.path.isdir(os.path.join(eval_path, x)), os.listdir(eval_path)))\n",
    "# if not eval_file in all_eval_files:\n",
    "#     multi_df = pd.DataFrame(columns=['data', 'seed', 'method', 'group', 'metric', 'value'])\n",
    "\n",
    "#     for data_name in datasets:\n",
    "#         for seed in seeds:\n",
    "#             eval_res = read_json('{}eval-{}-{}-mcc.json'.format(res_path+data_name+'/', seed, set_suffix))\n",
    "#             for method in methods:\n",
    "# #                 print(eval_res[method])\n",
    "                \n",
    "#                 for group in ['all', 'G0', 'G1']:\n",
    "#                     base = [data_name, seed, method, group]\n",
    "#                     for metric_i in group_eval_metrics:\n",
    "#                         multi_df.loc[multi_df.shape[0]] = base + [metric_i, eval_res[method][group][metric_i]]\n",
    "#                 for metric_i in overall_metrics:\n",
    "#                     multi_df.loc[multi_df.shape[0]] = [data_name, seed, method, 'all'] + [metric_i, eval_res[method]['all'][metric_i]]\n",
    "\n",
    "#     multi_df.to_csv(eval_path+eval_file, index=False)\n",
    "#     print('Save evaluation results at {}'.format(eval_path+eval_file))\n",
    "# else:\n",
    "#     multi_df = pd.read_csv(eval_path+eval_file)\n",
    "#     print('Read evaluation results from {}'.format(eval_path+eval_file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a91b8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extracting evaluation results from methods with a single model\n",
    "\n",
    "# methods = ['scc', 'scc', 'scc', 'omn', 'kam']\n",
    "# bases = ['one', 'kam', 'omn', 'one', 'one']\n",
    "\n",
    "# eval_file = 'Data{}-run{}-{}-{}.csv'.format(len(datasets), len(seeds), set_suffix, 'single')\n",
    "\n",
    "\n",
    "# all_eval_files = list(filter(lambda x: not os.path.isdir(os.path.join(eval_path, x)), os.listdir(eval_path)))\n",
    "# if not eval_file in all_eval_files:\n",
    "#     single_df = pd.DataFrame(columns=['data', 'seed', 'method', 'group', 'metric', 'value'])\n",
    "\n",
    "#     for data_name in datasets:\n",
    "#         cur_dir = 'intermediate/models/' + data_name +'/'\n",
    "#         for seed in seeds:\n",
    "#             for method, weight_base in zip(methods, bases):\n",
    "#                 eval_res = read_json('{}eval-{}-{}-{}-{}.json'.format(cur_dir, seed, set_suffix, method, weight_base))\n",
    "#                 method_eval = '{}-{}'.format(method.upper(), weight_base.upper())\n",
    "#                 for group in ['all', 'G0', 'G1']:\n",
    "#                     base = [data_name, seed, method_eval, group]\n",
    "#                     for metric_i in group_eval_metrics:\n",
    "#                         single_df.loc[single_df.shape[0]] = base + [metric_i, eval_res[method.upper()][group][metric_i]]\n",
    "#                 for metric_i in overall_metrics:\n",
    "#                     single_df.loc[single_df.shape[0]] = [data_name, seed, method_eval, 'all'] + [metric_i, eval_res[method.upper()]['all'][metric_i]]\n",
    "\n",
    "#     single_df.to_csv(eval_path+eval_file, index=False)\n",
    "#     print('Save evaluation results at {}'.format(eval_path+eval_file))\n",
    "# else:\n",
    "#     single_df = pd.read_csv(eval_path+eval_file)\n",
    "#     print('Read evaluation results from {}'.format(eval_path+eval_file))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5e7702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge and normalize results \n",
    "# eval_file = 'Data{}-run{}-{}.csv'.format(len(datasets), len(seeds), set_suffix)\n",
    "\n",
    "# all_eval_files = list(filter(lambda x: not os.path.isdir(os.path.join(eval_path, x)), os.listdir(eval_path)))\n",
    "# if not eval_file in all_eval_files:\n",
    "# #     eval_df = pd.concat([multi_df, single_df])\n",
    "# #     eval_df.reset_index(drop=True, inplace=True)\n",
    "#     eval_df = multi_df.copy()\n",
    "#     eval_df['norm_value'] = eval_df[['metric', 'value']].apply(lambda x: normalize_fairness_measures(x), axis=1)\n",
    "#     eval_df['norm_flag'] = eval_df[['metric', 'value']].apply(lambda x: add_vis_flag(x), axis=1)\n",
    "    \n",
    "#     eval_df.to_csv(eval_path+eval_file, index=False)\n",
    "#     print('Save evaluation results at {}'.format(eval_path+eval_file))\n",
    "# else:\n",
    "#     eval_df = pd.read_csv(eval_path+eval_file)\n",
    "#     print('Read evaluation results from {}'.format(eval_path+eval_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00ab988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = 'DI'\n",
    "# seeds = [88, 121, 433, 500, 1121, 50]\n",
    "# for name in datasets:\n",
    "#     for seed in seeds:\n",
    "# #         print(single_df.query('data==\"{}\" and seed=={} and setting==\"C\" and group==\"all\" and metric==\"{}\"'.format(name, seed, metric)), '\\n')\n",
    "# #         print(eval_df.query('data==\"{}\" and seed=={} and setting==\"SingleCC\" and group==\"all\" and metric==\"{}\"'.format(name, seed, metric)))\n",
    "# #         print('\\n')\n",
    "        \n",
    "# #         print(multi_df.query('data==\"{}\" and seed=={} and setting==\"F\" and group==\"all\" and metric==\"{}\"'.format(name, seed, metric)))\n",
    "# #         print(eval_df.query('data==\"{}\" and seed=={} and setting==\"MultiCC\" and group==\"all\" and metric==\"{}\"'.format(name, seed, metric)))\n",
    "# #         print('\\n')\n",
    "        \n",
    "#         print(multi_df.query('data==\"{}\" and seed=={} and setting==\"A\" and group==\"all\" and metric==\"{}\"'.format(name, seed, metric)))\n",
    "#         print(eval_df.query('data==\"{}\" and seed=={} and setting==\"no intervention\" and group==\"all\" and metric==\"{}\"'.format(name, seed, metric)))\n",
    "        \n",
    "#         print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced9c48",
   "metadata": {},
   "source": [
    "## Draw barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9b9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plots(df, output_name, vis_datasets, vis_metric, vis_settings, group_input=None,\n",
    "              legend_names=None, font_label=26, font_legend=18, \n",
    "              colors=['#ffffff', '#fffacd', '#3cb371','#20603d', '#0e6670'], bg_color = '#f3f3f3', x_tick_offset=6.3,\n",
    "              x_ticks=None, y_label=None, x_label=None, legend=True, legend_col=5, save_to_disc=True):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(10, 4), dpi=200)\n",
    "    input_df = df.copy()\n",
    "    bar_mean = []\n",
    "    bar_std = []\n",
    "    dash_filling = []\n",
    "    line_styles = []\n",
    "    x_bars = []\n",
    "    ind = 0\n",
    "    \n",
    "    for off_i, name in enumerate(vis_datasets):\n",
    "        vis_df = input_df[(input_df['data']==name) & (input_df['metric']==vis_metric) & (input_df['group']==group_input)].copy()\n",
    "        for setting_i in vis_settings:\n",
    "            set_df = vis_df[vis_df['method']==setting_i]\n",
    "            if set_df.shape[0] > 0:\n",
    "                y_values = np.array(set_df['norm_value'])\n",
    "                n_reverse = sum(np.array(set_df['norm_flag']))\n",
    "                if n_reverse > int(len(y_values) * 0.9): # majoirty of cases in which G0 has better outcomes\n",
    "                    dash_filling.append(True)\n",
    "                else:\n",
    "                    dash_filling.append(False)\n",
    "                \n",
    "                cur_mean = np.mean(y_values)\n",
    "                cur_std = np.std(y_values)\n",
    "                if cur_mean == 0:\n",
    "#                     print('++', name, setting_i, cur_mean)\n",
    "                    cur_mean = 0.01 # for visualization purpose so that the bar exists in the plot\n",
    "                    line_styles.append('solid')\n",
    "                elif cur_mean == 0.5 and vis_metric == 'BalAcc' and cur_std == 0:\n",
    "                    # dashed border\n",
    "                    line_styles.append('dashed')\n",
    "                elif cur_mean == 1.0 and vis_metric == 'DI' and cur_std == 0:\n",
    "                    line_styles.append('dashed')\n",
    "                else:\n",
    "                    line_styles.append('solid')\n",
    "                    \n",
    "                bar_mean.append(cur_mean)\n",
    "                bar_std.append(cur_std)\n",
    "            else: # no model is returned\n",
    "                dash_filling.append(False)\n",
    "                line_styles.append('dashed')\n",
    "                bar_mean.append(0)\n",
    "                bar_std.append(0)\n",
    "                \n",
    "            x_bars.append(ind+off_i*2)\n",
    "                \n",
    "            ind += 0.83\n",
    "    bplot = ax.bar(x_bars, bar_mean, yerr=bar_std)\n",
    "#     print('-->', vis_metric, bar_mean)\n",
    "    \n",
    "    n_bars = len(vis_settings)\n",
    "    for idx, patch in enumerate(bplot):\n",
    "        patch.set_facecolor(colors[idx % n_bars])\n",
    "        \n",
    "        if dash_filling[idx]:\n",
    "            patch.set_hatch('//')\n",
    "            patch.set_edgecolor(\"#cb4154\")\n",
    "        else:\n",
    "            patch.set_edgecolor(\"black\")\n",
    "        patch.set_linestyle(line_styles[idx])\n",
    "        \n",
    "    if legend_names:\n",
    "        legends = legend_names\n",
    "    else:\n",
    "        legends = vis_settings\n",
    "    # add labels for settings \n",
    "    for idx, setting_i, color_i, legend_i in zip(range(len(vis_settings)), vis_settings, colors, legends):\n",
    "        ax.bar(-2, 1, ec='black', fc=color_i, label=legend_i)\n",
    "\n",
    "    ax.set_facecolor(bg_color)\n",
    "    ax.yaxis.grid(True)\n",
    "    plt.xlim([-1, max(x_bars)+1])\n",
    "    plt.xticks([(x-1)*x_tick_offset+0.8 for x in range(1, len(vis_datasets)+1)])\n",
    "    if x_ticks:\n",
    "        locs, labels=plt.xticks();\n",
    "        plt.xticks(locs, x_ticks, horizontalalignment='center', fontsize=font_label-10, rotation=0);\n",
    "\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.yticks(fontsize=font_label);\n",
    "\n",
    "    if y_label:\n",
    "        plt.ylabel(y_label, fontsize=font_label)\n",
    "\n",
    "    if x_label:\n",
    "        plt.xlabel(x_label, fontsize=font_label)\n",
    "\n",
    "    if legend:\n",
    "        plt.legend(bbox_to_anchor=(0, 1, 1, 0), loc=\"lower center\", mode=\"expand\", ncol=legend_col, frameon=False, borderaxespad=0, handlelength=0.9, handletextpad=0.3, fontsize=font_label-7)\n",
    "\n",
    "    if save_to_disc:\n",
    "        plt.savefig(output_name, bbox_inches=\"tight\")\n",
    "        print('Bar plot is saved at ', output_name)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cef33f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path = repo_dir+ 'intermediate/plots/'\n",
    "if 'sort_' in eval_suffix or 'min_' in eval_suffix:\n",
    "    plot_path = plot_path + 'err/'\n",
    "    make_folder(plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df2f3422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO get the table for SR for all the datasets compareing MCC best variants with SEP, SCC+K, OMN, CAP for the usefulness of MCC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad522d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/LR/G0/LR-multi-SR-min_g0-0.5.png\n",
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/LR/G0/LR-multi-BalAcc-min_g0-0.5.png\n",
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/LR/G0/LR-single-SR-min_g0-0.5.png\n",
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/LR/G0/LR-single-BalAcc-min_g0-0.5.png\n",
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/TR/G0/TR-multi-SR-min_g0-0.5.png\n",
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/TR/G0/TR-multi-BalAcc-min_g0-0.5.png\n",
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/TR/G0/TR-single-SR-min_g0-0.5.png\n",
      "Bar plot is saved at  /Users/keyang/Projects/PubRepo/NonInvasiveTool4FairML/intermediate/plots/err/TR/G0/TR-single-BalAcc-min_g0-0.5.png\n"
     ]
    }
   ],
   "source": [
    "group_input = eval_suffix.split('-')[1].split('_')[1].upper()\n",
    "\n",
    "exp_datasets = ['credit', 'cardio', 'meps16', 'lsac', 'bank', 'ACSH', 'ACSP', 'ACSI', 'ACSE', 'ACSM']\n",
    "exp_ticks = ['Credit', 'Cardio', 'MEPS', 'LSAC', 'Bank', 'ACSH', 'ACSP', 'ACSI', 'ACSE', 'ACSM']\n",
    "\n",
    "mcc_setttings = ['ORIG', 'SEP', 'MCC-MIN', 'MCC-W1', 'MCC-W2', 'SCC-KAM']\n",
    "mcc_legends = ['ORIG', 'SEP', 'MCC-MIN', 'MCC-W1', 'MCC-W2', 'SCC+K']\n",
    "mcc_colors = ['#ffffff', '#fffacd', '#3cb371','#20603d', '#0e6670', '#2e8b57']\n",
    "\n",
    "scc_settings = ['ORIG', 'OMN-ONE', 'KAM-ONE', 'SCC-ONE', 'SCC-KAM']\n",
    "scc_legends = ['ORIG', 'OMN', 'KAM', 'SCC', 'SCC+K']\n",
    "scc_colors = ['#ffffff', '#fffacd', '#f4ca16', '#9dc209', '#2e8b57']\n",
    "\n",
    "# eval_metrics = ['BalAcc', 'DI', 'AvgOddsDiff', 'EQDiff', 'FPRDiff', 'FNRDiff', 'ERRDiff']\n",
    "\n",
    "eval_metrics = ['SR', 'BalAcc']\n",
    "\n",
    "for model_name in models:\n",
    "    if model_name == 'TR':\n",
    "        scc_colors = ['#ffffff', '#fffacd', '#ffc0cb', '#f4ca16', '#9dc209', '#2e8b57'] # '#ffdab9', '#006400'\n",
    "        scc_settings = ['ORIG', 'OMN-ONE', 'CAP-ONE', 'KAM-ONE', 'SCC-ONE', 'SCC-KAM']\n",
    "        scc_legends = ['ORIG', 'OMM', 'CAP', 'KAM', 'SCC', 'SCC+K']\n",
    "        \n",
    "        x_tick_sets = [7.2, 7.2]\n",
    "    else:\n",
    "        x_tick_sets = [7.2, 6.3]\n",
    "    vis_df = eval_df.query('model==\"{}\"'.format(model_name))\n",
    "    exp_path = plot_path+model_name+'/'+group_input + '/'\n",
    "    make_folder(exp_path)\n",
    "    \n",
    "    for settings_i, colors_i, setting_name, legend_i, x_tick_set in zip([mcc_setttings, scc_settings], [mcc_colors, scc_colors], ['multi', 'single'], [mcc_legends, scc_legends], x_tick_sets):\n",
    "#         print(vis_df.shape[0])\n",
    "        for exp_metric in eval_metrics:\n",
    "            output_name = '{}{}-{}-{}{}.png'.format(exp_path, model_name, setting_name, exp_metric, eval_suffix)\n",
    "            bar_plots(vis_df, output_name, exp_datasets, exp_metric, settings_i, group_input=group_input, x_ticks=exp_ticks, colors=colors_i, \n",
    "                      legend_names=legend_i, x_tick_offset=x_tick_set,\n",
    "                      legend_col=len(settings_i),\n",
    "                      save_to_disc=True)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07c1fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exp_name = 'lr'\n",
    "\n",
    "# # initiate a new folder for the plots\n",
    "# # exp_path = plot_path+exp_name+'/'\n",
    "# exp_path = plot_path+exp_name+'/'\n",
    "# make_folder(exp_path)\n",
    "\n",
    "# vis_df = eval_df.query('model==\"{}\"'.format(exp_name.upper()))\n",
    "\n",
    "# exp_settings = ['ORIG', 'SEP', 'MCC-MIN', 'MCC-W2'] #'MCC-W1', \n",
    "# # exp_settings = ['ORIG', 'OMN-ONE', 'KAM-ONE', 'SCC-KAM']\n",
    "\n",
    "# # exp_settings = ['OMN-ONE', 'SCC-OMN', 'KAM-ONE', 'SCC-KAM']\n",
    "\n",
    "# exp_datasets = ['credit', 'cardio', 'meps16', 'lsac', 'bank', 'ACSH', 'ACSP', 'ACSI', 'ACSE', 'ACSM']\n",
    "# exp_ticks = ['Credit', 'Cardio', 'MEPS', 'LSAC', 'Bank', 'ACSH', 'ACSP', 'ACSI', 'ACSE', 'ACSM']\n",
    "\n",
    "# exp_metric = 'BalAcc'\n",
    "# bar_plots(vis_df, exp_name, exp_datasets, exp_metric, exp_settings, x_ticks=exp_ticks, plot_path=exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed7324bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_metric = 'DI'\n",
    "# bar_plots(vis_df, exp_name, exp_datasets, exp_metric, exp_settings, x_ticks=exp_ticks, plot_path=exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55278a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_metric = 'FPRDiff'\n",
    "# bar_plots(vis_df, exp_name, exp_datasets, exp_metric, exp_settings, x_ticks=exp_ticks, plot_path=exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f5b59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_metric = 'FNRDiff'\n",
    "# bar_plots(vis_df, exp_name, exp_datasets, exp_metric, exp_settings, x_ticks=exp_ticks, plot_path=exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a868e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_metric = 'ERRDiff'\n",
    "# bar_plots(vis_df, exp_name, exp_datasets, exp_metric, exp_settings, x_ticks=exp_ticks, plot_path=exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13329353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_metric = 'EQDiff'\n",
    "# bar_plots(vis_df, exp_name, exp_datasets, exp_metric, exp_settings, x_ticks=exp_ticks, plot_path=exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64f56b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_metric = 'AvgOddsDiff'\n",
    "# bar_plots(vis_df, exp_name, exp_datasets, exp_metric, exp_settings, x_ticks=exp_ticks, plot_path=exp_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38f771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
